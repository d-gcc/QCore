{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504febfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch import tensor, long\n",
    "#import torch, pathlib, os, scipy.io\n",
    "import numpy as np, pandas as pd, os, scipy.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset, ConcatDataset\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024794e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_same_size(a, size):\n",
    "    groups = np.split(a, np.arange(size,len(a),size))\n",
    "    return [(i) for i in groups if len(i)==size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(source,subject):\n",
    "    if source == 'DSA':\n",
    "        seriesList = []\n",
    "        labelsList = []\n",
    "\n",
    "        path = './data/DailyActivities/'\n",
    "        for activity in os.listdir(path):\n",
    "            pathSubject = path + activity +'/p' + str(subject)\n",
    "            for segment in os.listdir(pathSubject):\n",
    "                data = np.loadtxt(pathSubject + '/' + segment, delimiter=',')\n",
    "                seriesList.append(np.expand_dims(data, axis=0))\n",
    "                labelsList.append(int(activity[1:])-1)\n",
    "\n",
    "        inputData = np.concatenate(seriesList).transpose(0, 2, 1)\n",
    "        inputLabels = np.asarray(labelsList)\n",
    "\n",
    "        train_x, test_x, train_y, test_y = train_test_split(inputData, inputLabels, train_size=0.8, random_state=1)\n",
    "\n",
    "        stack = train_x.reshape(train_x.shape[0], -1)\n",
    "\n",
    "    elif source == 'USC':\n",
    "        sizeSeries = 500\n",
    "        seriesList = []\n",
    "        labelsList = []\n",
    "\n",
    "        path = './data/USC-HAD/Subject'+ str(subject)\n",
    "        for file in os.listdir(path):\n",
    "            mat = scipy.io.loadmat(path + '/' + file)\n",
    "            try:\n",
    "                act = mat['activity_number'][0]\n",
    "            except:\n",
    "                act = mat['activity_numbr'][0]\n",
    "\n",
    "            series = split_same_size(mat['sensor_readings'], sizeSeries)\n",
    "            activitySubject = np.concatenate(series).reshape(len(series),sizeSeries,-1)\n",
    "            activitylabel = np.repeat(int(act)-1, activitySubject.shape[0])\n",
    "            seriesList.append(activitySubject)\n",
    "            labelsList.append(activitylabel)\n",
    "\n",
    "        inputData = np.concatenate(seriesList).transpose(0, 2, 1)\n",
    "        inputLabels = np.concatenate(labelsList)\n",
    "\n",
    "        train_x, test_x, train_y, test_y = train_test_split(inputData, inputLabels, train_size=0.8, random_state=1)\n",
    "\n",
    "        stack = train_x.reshape(train_x.shape[0], -1)\n",
    "    return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c599b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 30\n",
    "\n",
    "for data_source in ['DSA','USC']:\n",
    "    if data_source == 'DSA':\n",
    "        domains = 8\n",
    "    elif data_source == 'USC':\n",
    "        domains = 14\n",
    "    for subj in range(1,domains+1):\n",
    "        stack = data(data_source, subj)\n",
    "        kmeans = KMeans(n_clusters=clusters).fit(stack)\n",
    "        #print(kmeans.labels_)\n",
    "        kmeans.predict(stack)\n",
    "        #print(kmeans.cluster_centers_.shape)\n",
    "        \n",
    "        # Loop over all clusters and find index of closest point to the cluster center and append to closest_pt_idx list.\n",
    "        closest_pt_idx = []\n",
    "        for i, iclust in enumerate(range(kmeans.n_clusters)):\n",
    "            # get all points assigned to each cluster:\n",
    "            cluster_pts = stack[kmeans.labels_ == iclust]\n",
    "            # get all indices of points assigned to this cluster:\n",
    "            cluster_pts_indices = np.where(kmeans.labels_ == iclust)[0]\n",
    "\n",
    "            cluster_cen = kmeans.cluster_centers_[iclust]\n",
    "            min_idx = np.argmin([euclidean(stack[idx], cluster_cen) for idx in cluster_pts_indices])\n",
    "\n",
    "            # Testing:    \n",
    "            #print('closest point to cluster center: ', cluster_pts[min_idx])\n",
    "            #print(str(i)+','+str(cluster_pts_indices[min_idx]))\n",
    "            #print('  ', aa[cluster_pts_indices[min_idx]])\n",
    "            closest_pt_idx.append(cluster_pts_indices[min_idx])\n",
    "        df = pd.DataFrame(closest_pt_idx)\n",
    "        df.to_csv('./Cores/KMeans_' + data_source + '_' +str(subj) +'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Compression",
   "language": "python",
   "name": "compression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
